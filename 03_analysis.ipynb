{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVCI analysis\n",
    "\n",
    "> **API**: The utilization scores for each site are computed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import shapely\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.cluster.vq import kmeans2, whiten\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "from evci_tool.config import *\n",
    "from evci_tool.model import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def run_episode(m,s,t,g,s_df,txt,OUTPUT_PATH,corridor):\n",
    "    \"This function runs a full episode of analysis on a set of sites.\"\n",
    "    \n",
    "    print('\\n' + txt.capitalize() + ' Analysis')\n",
    "    print('________________\\n')\n",
    "    total = s_df.shape[0]\n",
    "    \n",
    "    #s_df = s_df[s_df['year 1'] == 1]\n",
    "    #s_df = s_df.reset_index(drop=True)\n",
    "    \n",
    "    Nc = s_df.shape[0]\n",
    "    print(f'Number of sites: {Nc}/{total}')\n",
    "\n",
    "    #@title Compute scores\n",
    "\n",
    "    backoff_factor = 2 #@param {type:\"slider\", min:1, max:5, step:1}\n",
    "\n",
    "    u_df = run_analysis(m,s,t,g,s_df,backoff_factor=backoff_factor)\n",
    "\n",
    "    print(f'Total capex charges = INR Cr {sum(u_df.capex)/1e7:.2f}')\n",
    "    print(f'Total opex charges = INR Cr {sum(u_df.opex)/1e7:.2f}')\n",
    "    print(f'Total Margin = INR Cr {sum(u_df.margin)/1e7:.2f}')        \n",
    "\n",
    "    #@title Prepare data\n",
    "    s_u_df = s_df.copy()\n",
    "\n",
    "    s_u_df['utilization'] = u_df.utilization\n",
    "    s_u_df['unserviced'] = u_df.unserviced\n",
    "    s_u_df['capex'] = u_df.capex\n",
    "    s_u_df['opex'] = u_df.opex\n",
    "    s_u_df['margin'] = u_df.margin\n",
    "    s_u_df['max vehicles'] = u_df['max vehicles']\n",
    "    s_u_df['estimated vehicles'] = u_df['estimated vehicles']\n",
    "\n",
    "    #@title Save initial analysis to Excel\n",
    "    output_df = s_u_df.copy()\n",
    "    output_df.drop('geometry', axis=1, inplace=True)\n",
    "    output_df.to_excel(OUTPUT_PATH + '/' + txt + '_evci_analysis.xlsx')\n",
    "    return s_u_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Arguments`:\n",
    "\n",
    "1. `m`: dataframe of model parameters (from model.xlsx)\n",
    "2. `s`: dataframe of sites (from sites.xlsx)\n",
    "3. `t`: dataframe of traffic profile (from traffic.xlsx)\n",
    "4. `g`: dataframe of grid parameters (from grid.xlsx)\n",
    "5. `s_df`: pre-processed geopandas dataframe with each point stored as shapely point object\n",
    "6. `txt`: a string that identifies the episode (e.g. initial, final, with_cluster etc)\n",
    "7. `OUTPUT_PATH`: the directory path where the generated output files will be stored\n",
    "8. `corridor`: a string that identifies the corridor being analyzed (e.g. chandigarh_leh)\n",
    "\n",
    "`Returns`: \n",
    "\n",
    "A pandas dataframe: `s_u_df` is a dataframe with computed utilization values for each site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def analyze_sites(corridor:str, cluster:bool=False):\n",
    "    \"The function analyzes sites specified as part of a corridor.\"\n",
    "\n",
    "    #@title Read data from excel sheets\n",
    "    model,site,traffic,grid, INPUT_PATH, OUTPUT_PATH = setup_and_read_data(corridor)\n",
    "\n",
    "    #@title Read required data sheets only\n",
    "    df = gpd.read_file(INPUT_PATH + '/shape_files/' + corridor + '.shp')\n",
    "\n",
    "    data = site['sites']\n",
    "    data['Name'] = data['Name']\n",
    "    data['Latitude'] = pd.to_numeric(data['Latitude'])\n",
    "    data['Longitude'] = pd.to_numeric(data['Longitude'])\n",
    "    data['geometry'] = [shapely.geometry.Point(xy) for xy in \n",
    "                        zip(data['Longitude'], data['Latitude'])]\n",
    "\n",
    "    data_df = {}\n",
    "\n",
    "    data_df = gpd.GeoDataFrame(data, geometry=data['geometry'])\n",
    "\n",
    "    s_df = pd.DataFrame(columns=['Name',\n",
    "                                'Latitude', 'Longitude',\n",
    "                                'Traffic congestion',\n",
    "                                'year 1',\n",
    "                                'kiosk hoarding',\n",
    "                                'hoarding margin',\n",
    "                                'geometry'])\n",
    "\n",
    "    s_df = s_df.reset_index(drop=True)\n",
    "\n",
    "    for i in range(data_df.shape[0]):\n",
    "        s_df.loc[i] = [\n",
    "           data_df.loc[i].Name, \n",
    "           data_df.loc[i].Latitude, \n",
    "           data_df.loc[i].Longitude, \n",
    "           data_df.loc[i]['Traffic congestion'],\n",
    "           data_df.loc[i]['Year for Site recommendation'],\n",
    "           data_df.loc[i]['Hoarding/Kiosk (1 is yes & 0 is no)'],\n",
    "           data_df.loc[i]['Hoarding margin'],\n",
    "           data_df.loc[i].geometry\n",
    "        ] \n",
    "    \n",
    "    s_u_df = run_episode(model,site,traffic,grid,s_df,'initial',OUTPUT_PATH, corridor)\n",
    "\n",
    "    #@title Threshold and cluster\n",
    "    if cluster:\n",
    "        clustering_candidates = s_u_df[(s_u_df.utilization <= 0.2) & (s_u_df['year 1'] == 1)]\n",
    "        points = np.array((clustering_candidates.apply(lambda x: list([x['Latitude'], x['Longitude']]),axis=1)).tolist())\n",
    "        Z = linkage (points, method='complete', metric='euclidean');\n",
    "        plt.figure(figsize=(14,8))\n",
    "        dendrogram(Z);\n",
    "        max_d = 0.01\n",
    "        clusters = fcluster(Z, t=max_d, criterion='distance')\n",
    "        clustered_candidates = gpd.GeoDataFrame(clustering_candidates)\n",
    "        #base = grid_df.plot(color='none', alpha=0.2, edgecolor='black', figsize=(8,8))\n",
    "        #clustered_candidates.plot(ax=base, column=clusters, legend=True)\n",
    "\n",
    "    #@title Build final list of sites\n",
    "    confirmed_sites = s_u_df[s_u_df.utilization > 0.2]\n",
    "    if cluster:\n",
    "        val, ind = np.unique (clusters, return_index=True)\n",
    "        clustered_sites = clustered_candidates.reset_index(drop=True)\n",
    "        clustered_sites = clustered_sites.iloc[clustered_sites.index.isin(ind)]\n",
    "        final_list_of_sites = pd.concat([confirmed_sites, clustered_sites], axis=0)\n",
    "    else:\n",
    "        final_list_of_sites = confirmed_sites.copy()\n",
    "\n",
    "    if cluster:\n",
    "        s_u_df = run_episode(final_list_of_sites,'cluster',OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Arguments`:\n",
    "\n",
    "1. `corridor`: a string that identifies the corridor being analyzed (e.g. chandigarh_leh)\n",
    "2. `cluster`: a boolean flag that indicates whether clustering algorithm should be run. Default is `False`\n",
    "\n",
    "`Returns`:\n",
    "\n",
    "None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Analysis\n",
      "________________\n",
      "\n",
      "Number of sites: 48/48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 48/48 [00:00<00:00, 748425.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total capex charges = INR Cr 0.19\n",
      "Total opex charges = INR Cr 1.01\n",
      "Total Margin = INR Cr nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "analyze_sites('chandigarh_leh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
